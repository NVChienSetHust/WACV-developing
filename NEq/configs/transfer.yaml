# Project name for wandb logging
project_name: quantization_test
# Change run_dir to where you want to save the model
run_dir: ~/on_device_learning/WACV-2024-Ways-to-Select-Neurons-under-a-Budget-Constraint/saved-models
manual_seed: 0
resume: 0
# Change to 0 for single training and 1 for wandb sweep
wandb_sweep: 1

data_provider:
  dataset: cifar100
  root: /home/infres/vnguyen-23/dataset/
  resize_scale: 0.08
  color_aug: 0.4
  base_batch_size: 64
  n_worker: 10
  image_size: 128
  # Number of classes in target dataset for classifier head dimensions
  new_num_classes: 100

  # For c10, c100, vww, food101, pets and cub200 (except flowers102 because it already has a validation set)
  validation_percentage: 0.2 #(use to divide train set into train and validation)

  # Use these sets for the training process?: 1 for yes, 0 for no
  use_validation_for_velocity: 1 # set in config.py/update_config_from_wandb() function
  use_validation: 0

run_config:
  n_epochs: 50
  base_lr: 0.01 # Ael's policy: 0.125
  bs256_lr: null  # Notice: if this one is not None, it will overwrite base_lr
  warmup_epochs: 5
  warmup_lr: 0
  lr_schedule_name: cosine
  # wd
  weight_decay: 0
  no_wd_keys: ['norm', 'bias']
  # optimizer
  optimizer_name: sgd
  quantized: 1
  QAS: 1
  # optimizer_name: sgd_nomom
  # optimizer_name: sgd_scale
  momentum: 0
  # eval sparsely
  eval_per_epochs: 1
  test_per_epochs: 1

# tiny training config
  # partial blocks for fp32
  n_block_update: -1
  # grid search fine-tuning
  grid_output: null
  grid_ckpt_path: null
  
  bias_only: 0
  fc_only: 0
  fc_lr10: 0
net_config:
  # net_name: pre_trained_mbv2
  net_name: mcunet-5fps
  fine_tuning: true
  
# tiny training config
  cls_head: linear
  dropout: 0.
  mcu_head_type: fp

NEq_config:
  # Total number of parameters in the network
  total_num_params: 463216
  # total_num_params: 2189760
  # Ratio of total number of parameters to use as budget
  ratio: 1
  # Placeholder for budget expressed in maximal number of trainable parameters
  glob_num_params: 463216
  # glob_num_params: 599040
  velocity_mu: 0.5
  neuron_selection: random # chose between SU, velocity, random and full
  initialization: random # chose between SU, random and full

backward_config:  # for partial backward
  enable_backward_config: 1
  # n_bias_update: 31  # how many conv to update the bias
  # weight_update_ratio: 1-1-1-1-1-0.5-1-1  # how many weights along input channels are updated (also support int number)
  # manual_weight_idx: 27-30-33-36-39-42-45-48
  n_bias_update: 20  # how many conv to update the bias
  weight_update_ratio: 0-0.25-0.5-0.5-0-0  # how many percentage of weights along input channels are updated (also support int number)
  manual_weight_idx: 23-24-27-30-33-39 # Which layers will be updated weight

# ----------------configs from tiny training---------------------------
  weight_select_criteria: magnitude+
  # mbv2
  pw1_weight_only: 0  # only update the weight of the first pointwise conv (since it has smaller input act.)
  quantize_gradient: 0
  freeze_fc: 0
  train_scale: 0